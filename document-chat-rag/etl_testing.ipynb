{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JEPDocument Chatbot - ETL Pipeline Testing\n",
    "\n",
    "\n",
    "This notebook provides a comprehensive testing environment for the ETL (Extract, Transform, Load) pipeline of the legal document chatbot system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Import required modules\n",
    "from src.etl.excel_reader import read_excel_file, validate_excel_format\n",
    "from src.etl.pdf_downloader import download_pdfs\n",
    "from src.etl.docling_processor import process_document\n",
    "from src.etl.embedding_generator import generate_embeddings_for_chunks\n",
    "from src.database.vector_store import initialize_database, VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Excel Reader Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:25:23,018 - src.etl.excel_reader - ERROR - Error validating Excel format: [Errno 2] No such file or directory: 'data/excel/document_links.xlsx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file format is valid: False\n",
      "Excel file format is not valid. Please check the file.\n"
     ]
    }
   ],
   "source": [
    "# Test Excel reader functionality\n",
    "# Replace with the path to your Excel file\n",
    "excel_file_path = \"data/excel/Reporte_providencias_ia_2024-10-09_06_49_57.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Validate Excel format\n",
    "    is_valid = validate_excel_format(excel_file_path)\n",
    "    print(f\"Excel file format is valid: {is_valid}\")\n",
    "\n",
    "    # Read Excel file\n",
    "    if is_valid:\n",
    "        pdf_data = read_excel_file(excel_file_path)\n",
    "        print(f\"Found {len(pdf_data)} PDF links in the Excel file\")\n",
    "        \n",
    "        # Display first few entries\n",
    "        for i, pdf_info in enumerate(pdf_data[:3]):\n",
    "            print(f\"\\nEntry {i+1}:\")\n",
    "            print(f\"  URL: {pdf_info['url']}\")\n",
    "            print(f\"  Filename: {pdf_info['filename']}\")\n",
    "            print(f\"  Row Index: {pdf_info['row_index']}\")\n",
    "            print(f\"  Metadata keys: {list(pdf_info['metadata'].keys())}\")\n",
    "    else:\n",
    "        print(\"Excel file format is not valid. Please check the file.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Excel file not found at {excel_file_path}. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Downloader Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PDF downloader functionality\n",
    "if 'pdf_data' in locals():\n",
    "    try:\n",
    "        # Download first 3 PDFs for testing\n",
    "        test_pdf_data = pdf_data[:3]\n",
    "        \n",
    "        download_result = download_pdfs(test_pdf_data, download_dir=\"data/raw\")\n",
    "        \n",
    "        print(f\"Total files processed: {download_result['total_files']}\")\n",
    "        print(f\"Successful downloads: {len(download_result['successful_downloads'])}\")\n",
    "        print(f\"Failed downloads: {len(download_result['failed_downloads'])}\")\n",
    "        \n",
    "        # Display successful downloads\n",
    "        for download in download_result['successful_downloads']:\n",
    "            print(f\"  - {download['filename']} downloaded to {download['filepath']}\")\n",
    "        \n",
    "        # Display failed downloads\n",
    "        for download in download_result['failed_downloads']:\n",
    "            print(f\"  - {download['filename']} failed: {download['error']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDFs: {str(e)}\")\n",
    "else:\n",
    "    print(\"PDF data not available. Please run the Excel reader test first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Processing with Docling Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Docling processor functionality\n",
    "if 'download_result' in locals() and download_result['successful_downloads']:\n",
    "    try:\n",
    "        # Process the first successfully downloaded PDF\n",
    "        first_pdf = download_result['successful_downloads'][0]\n",
    "        pdf_file_path = first_pdf['filepath']\n",
    "        \n",
    "        print(f\"Processing document: {pdf_file_path}\")\n",
    "        \n",
    "        # Process the document\n",
    "        processing_result = process_document(pdf_file_path)\n",
    "        \n",
    "        print(f\"Processing status: {processing_result.status}\")\n",
    "        if processing_result.status == 'success':\n",
    "            print(f\"Generated {len(processing_result.chunks)} chunks\")\n",
    "            \n",
    "            # Display first few chunks\n",
    "            for i, chunk in enumerate(processing_result.chunks[:3]):\n",
    "                print(f\"\\nChunk {i+1}:\")\n",
    "                print(f\"  Content preview: {chunk.content[:200]}...\")\n",
    "                print(f\"  Page numbers: {chunk.page_numbers}\")\n",
    "                print(f\"  Section title: {chunk.section_title}\")\n",
    "                print(f\"  Metadata: {chunk.chunk_metadata}\")\n",
    "        else:\n",
    "            print(f\"Processing failed: {processing_result.error_message}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {str(e)}\")\n",
    "else:\n",
    "    print(\"No successful downloads available. Please run the PDF downloader test first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding generation functionality\n",
    "if 'processing_result' in locals() and processing_result.status == 'success':\n",
    "    try:\n",
    "        # Convert chunks to the format expected by embedding generator\n",
    "        chunks_for_embedding = [\n",
    "            {\n",
    "                'content': chunk.content,\n",
    "                'chunk_metadata': chunk.chunk_metadata\n",
    "            }\n",
    "            for chunk in processing_result.chunks\n",
    "        ]\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(chunks_for_embedding)} chunks\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        chunks_with_embeddings = generate_embeddings_for_chunks(chunks_for_embedding)\n",
    "        \n",
    "        print(f\"Generated embeddings for {len(chunks_with_embeddings)} chunks\")\n",
    "        \n",
    "        # Display embedding information\n",
    "        for i, chunk in enumerate(chunks_with_embeddings[:3]):\n",
    "            print(f\"\\nChunk {i+1} embedding:\")\n",
    "            print(f\"  Embedding dimension: {len(chunk['embedding'])}\")\n",
    "            print(f\"  Embedding preview: {chunk['embedding'][:5]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings: {str(e)}\")\n",
    "else:\n",
    "    print(\"No processed chunks available. Please run the document processing test first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Store Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector store functionality\n",
    "try:\n",
    "    # Initialize database\n",
    "    initialize_database()\n",
    "    print(\"Database initialized successfully\")\n",
    "    \n",
    "    # Create vector store instance\n",
    "    vector_store = VectorStore()\n",
    "    \n",
    "    # Store document metadata\n",
    "    if 'first_pdf' in locals():\n",
    "        document_id = vector_store.store_document(\n",
    "            filename=first_pdf['filename'],\n",
    "            source_url=first_pdf['url'],\n",
    "            metadata=first_pdf['metadata']\n",
    "        )\n",
    "        print(f\"Stored document with ID: {document_id}\")\n",
    "        \n",
    "        # Store chunks with embeddings\n",
    "        if 'chunks_with_embeddings' in locals():\n",
    "            vector_store.store_chunks(document_id, chunks_with_embeddings)\n",
    "            print(f\"Stored {len(chunks_with_embeddings)} chunks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Processing Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query processing functionality\n",
    "try:\n",
    "    from src.rag.query_processor import process_query\n",
    "    \n",
    "    # Test query\n",
    "    test_query = \"What are the key points in the legal document?\"\n",
    "    \n",
    "    print(f\"Processing query: {test_query}\")\n",
    "    \n",
    "    # Process query\n",
    "    query_result = process_query(test_query)\n",
    "    \n",
    "    print(f\"Query result:\")\n",
    "    print(f\"  Found {query_result['chunk_count']} relevant chunks\")\n",
    "    print(f\"  Response: {query_result['response']}\")\n",
    "    \n",
    "    # Display chunk information\n",
    "    for i, chunk in enumerate(query_result['chunks'][:3]):\n",
    "        print(f\"\\nRelevant Chunk {i+1} (similarity: {chunk['similarity']:.3f}):\")\n",
    "        print(f\"  Content preview: {chunk['content'][:200]}...\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Query processor module not found. Please check if it's implemented.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing query processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete ETL Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete ETL pipeline\n",
    "def run_complete_etl_pipeline(excel_file_path, max_documents=3):\n",
    "    \"\"\"Run the complete ETL pipeline for testing.\"\"\"\n",
    "    print(\"=== Running Complete ETL Pipeline ===\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Read Excel file\n",
    "        print(\"\\n1. Reading Excel file...\")\n",
    "        pdf_data = read_excel_file(excel_file_path)\n",
    "        print(f\"   Found {len(pdf_data)} PDF links\")\n",
    "        \n",
    "        # Limit to max_documents for testing\n",
    "        pdf_data = pdf_data[:max_documents]\n",
    "        \n",
    "        # Step 2: Download PDFs\n",
    "        print(\"\\n2. Downloading PDFs...\")\n",
    "        download_result = download_pdfs(pdf_data, download_dir=\"data/raw\")\n",
    "        print(f\"   Successfully downloaded {len(download_result['successful_downloads'])} PDFs\")\n",
    "        \n",
    "        # Step 3: Process documents and generate embeddings\n",
    "        print(\"\\n3. Processing documents and generating embeddings...\")\n",
    "        vector_store = VectorStore()\n",
    "        \n",
    "        for i, download in enumerate(download_result['successful_downloads']):\n",
    "            print(f\"   Processing document {i+1}/{len(download_result['successful_downloads'])}: {download['filename']}\")\n",
    "            \n",
    "            try:\n",
    "                # Process document with Docling\n",
    "                processing_result = process_document(download['filepath'])\n",
    "                \n",
    "                if processing_result.status == 'success':\n",
    "                    print(f\"     Generated {len(processing_result.chunks)} chunks\")\n",
    "                    \n",
    "                    # Convert chunks for embedding generation\n",
    "                    chunks_for_embedding = [\n",
    "                        {\n",
    "                            'content': chunk.content,\n",
    "                            'chunk_metadata': chunk.chunk_metadata\n",
    "                        }\n",
    "                        for chunk in processing_result.chunks\n",
    "                    ]\n",
    "                    \n",
    "                    # Generate embeddings\n",
    "                    chunks_with_embeddings = generate_embeddings_for_chunks(chunks_for_embedding)\n",
    "                    \n",
    "                    # Store document metadata\n",
    "                    document_id = vector_store.store_document(\n",
    "                        filename=download['filename'],\n",
    "                        source_url=download['url'],\n",
    "                        metadata=download['metadata']\n",
    "                    )\n",
    "                    \n",
    "                    # Store chunks with embeddings\n",
    "                    vector_store.store_chunks(document_id, chunks_with_embeddings)\n",
    "                    print(f\"     Stored document and {len(chunks_with_embeddings)} chunks\")\n",
    "                else:\n",
    "                    print(f\"     Processing failed: {processing_result.error_message}\")\n",
    "            except Exception as e:\n",
    "                print(f\"     Error processing document: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n=== ETL Pipeline Complete ===\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ETL pipeline: {str(e)}\")\n",
    "\n",
    "# Run the complete ETL pipeline (uncomment to execute)\n",
    "# run_complete_etl_pipeline(\"data/excel/document_links.xlsx\", max_documents=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
